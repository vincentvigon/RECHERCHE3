{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ddqn et popup.ipynb","provenance":[],"authorship_tag":"ABX9TyM4Y+MSqSKN4tO2JoNgVROp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1KWRseJgOrM6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkEsVrBiO7C-"},"source":["## Popup"]},{"cell_type":"code","metadata":{"id":"VAdkGVnZO8u4"},"source":["import numpy as np\n","import time\n","from typing import List,Dict,Set\n","from abc import ABC, abstractmethod\n","from collections import deque\n","pp=print\n","import copy\n","\n","#todo metric multi-dim\n","\n","class Param(ABC):\n","    def __init__(self):\n","        self.name=\"toto\"\n","        self.value=0\n","    @abstractmethod\n","    def perturb(self):\n","        pass\n","\n","\n","class History:\n","\n","    def __init__(self):\n","\n","        self.localTime_previous_cumulated_values = 0\n","        self.score_times =  []\n","        self.metrics_times:Dict[str,List[float]] = {}\n","        self.metrics_flags: Dict[str, Set[int]] = {}\n","        self.metrics_values:Dict[str,List[float]]={}\n","        self.localTime_isActive=False\n","\n","    \"\"\"Important: il faut démarrer le local time dès que l'agent est actif. Puis l'arrêter dès qu'il est inactif \"\"\"\n","    def start_local_time(self):\n","        self.localTime_restart=time.time()\n","        assert not self.localTime_isActive , \"on a oublié d'arrêter le localTime\"\n","        self.localTime_isActive =True\n","\n","    def stop_local_time(self):\n","        self.localTime_previous_cumulated_values=self.get_local_time()\n","        assert self.localTime_isActive , \"le localTime n'est pas actif\"\n","        self.localTime_isActive =False\n","\n","    def get_local_time(self):\n","        if not self.localTime_isActive: #c'est logique\n","            return self.localTime_previous_cumulated_values\n","        else:\n","            return time.time()-self.localTime_restart+self.localTime_previous_cumulated_values\n","\n","\n","    def record_metric(self, name,value,addTag=False):\n","        assert is_number(value) ,f\"Error when recording {name} value must be a number, but is: {value} of type {type(value)}\"\n","        metric_times=self.metrics_times.get(name,[])\n","        metric_values=self.metrics_values.get(name,[])\n","        metric_times.append(self.get_local_time())\n","        metric_values.append(value)\n","        self.metrics_times[name]=metric_times\n","        self.metrics_values[name]=metric_values\n","\n","        if addTag:\n","            metric_timeFlags=self.metrics_flags.get(name, set())\n","            metric_timeFlags.add(len(metric_times)-1)\n","            self.metrics_flags[name]=metric_timeFlags\n","\n","\n","def is_number(x):\n","    return isinstance(x, (int, float, np.float32, np.float64, np.int32, np.int64))\n","\n","\n","\n","\n","class Abstract_Agent(ABC):\n","\n","    @abstractmethod\n","    def optimize_return_score(self)->float:\n","        pass\n","\n","\n","    @abstractmethod\n","    def set_famparams(self, dico):\n","        pass\n","\n","    @abstractmethod\n","    def get_famparams(self):\n","        pass\n","\n","    @abstractmethod\n","    def perturb_famparams(self):\n","        pass\n","\n","    \"\"\" faut-il plutot demandé un dico à l'utilisateur ? \"\"\"\n","    @abstractmethod\n","    def set_weights(self,weights:List):\n","        pass\n","\n","    @abstractmethod\n","    def get_copy_of_weights(self)->List:\n","        pass\n","\n","    #facultatif\n","\n","    def weights_info_to_register(self)->Dict[str,float]:\n","        # by default an empty dictionnary\n","        return dict()\n","\n","    def to_register_at_period_end(self)->Dict[str,float]:\n","        #by default an empty dictionnary\n","        return dict()\n","\n","    def on_overfitting(self):\n","        #appeler quand le score de validation est très infieur au score train\n","        #typiquement: on augmente les famparam qui baissent le sur-apprentissage\n","        pass\n","\n","class Agent_wraper:\n","\n","    def __init__(self, name, agent:Abstract_Agent, max_nb_best):\n","        self.name=name\n","        self.agent=agent\n","        self.max_nb_best=max_nb_best\n","\n","        self.best_weights = deque(maxlen=max_nb_best)\n","        self.best_score = None\n","        self.best_famparams: Dict[str, Param]\n","\n","        self.mutation_names=[]\n","        self.score=-float(\"inf\")\n","\n","    def get_name_suffixed(self):\n","        res=self.name\n","        for mut in self.mutation_names:\n","            res+=\"_\"+mut\n","        return res\n","\n","    def save_at_best(self,new_best_score:float):\n","        self.best_weights.append(self.agent.get_copy_of_weights())\n","        self.best_score=new_best_score\n","        self.best_famparams=copy.deepcopy(self.agent.get_famparams())\n","\n","    def mean_of_best_weights(self):\n","        nb_wei = len(self.best_weights[0])\n","        nb_best = len(self.best_weights)\n","        new_wei_list = []\n","        for j in range(nb_wei):\n","            shape = self.best_weights[0][j].shape\n","            res = np.zeros(shape)\n","            for i in range(nb_best):\n","                res += self.best_weights[i][j]\n","            res /= nb_best\n","            new_wei_list.append(res)\n","        return new_wei_list\n","\n","\n","    #exploitation\n","    def load_from_another(self,other_agent_w:'Agent_wraper'):\n","\n","        \"\"\"les weights (ou learning variable) d'un réseau sont toujours une liste ou un tuple de tenseur\"\"\"\n","        new_wei_list=other_agent_w.mean_of_best_weights()\n","\n","        self.agent.set_weights(new_wei_list)\n","        self.agent.set_famparams(copy.deepcopy(other_agent_w.best_famparams))\n","\n","        \"\"\"\n","         Ici, c'est pas l'idéal: on attribut comme best_score e best_score du l'other_agent.\n","         Or le mutant devrait faire ses preuves avant de passer dans la liste des premier.\n","         Mais on veut éviter qu'il ne soit remuter immédiatement après (il faudrait mettre en place un mécanisme de bonus pour les jeunes mutants)\n","         \"\"\"\n","        self.score=other_agent_w.best_score\n","        \"\"\"ne pas oublier de vider la liste des best_weights. Les anciens peuvent être assez différents : le changement de famparams peu induire des changement brutal de poids (ex: coef de pénalisation)\"\"\"\n","        self.best_weights=deque(maxlen=self.max_nb_best)\n","        self.best_weights.append(new_wei_list)\n","\n","\n","class Family_trainer:\n","    instance_count=0\n","    def __init__(self,\n","                 agents:List[Abstract_Agent],\n","                 ratio_weak=0.4,\n","                 ratio_strong=0.4,\n","                 nb_bestweights_averaged=5,\n","                 color=\"k\",\n","                 min_interessant_score=-float(\"inf\"),  # en-dessous de ce score, on mute aléatoirement.\n","                 name=None,\n","                 # à la fin de chaque période on fait les mutations\n","                 # une période dure \"periode_duration\"\n","                 periode_duration=10,\n","                 # \"step\" = un appelle de optimize_return_score\n","                 period_duration_unity=\"step\"  #ou \"seconde\"\n","                 ):\n","\n","        Family_trainer.instance_count+=1\n","\n","        self.min_interessant_score=min_interessant_score\n","        self.name=name if name is not None else \"fam_\"+str(Family_trainer.instance_count)\n","        self.periode_duration=periode_duration\n","        assert period_duration_unity==\"step\" or period_duration_unity==\"seconde\", \"period_duration_unity must be 'step' or 'minute'\"\n","        self.period_duration_unity=period_duration_unity\n","\n","        self.ratio_weak=ratio_weak\n","        self.nb_bestweights_averaged=nb_bestweights_averaged\n","        self.ratio_strong=ratio_strong\n","        self.color=color\n","\n","        self._period_count=-1\n","\n","        #un dico car on voudrait pouvoir supprimer des agents\n","        self.agents:Dict[str,Agent_wraper] = {}\n","        self.history=History()\n","\n","        for i,agent in enumerate(agents):\n","            agent_name=str(i)\n","            agent_w = Agent_wraper(agent_name, agent, self.nb_bestweights_averaged)\n","            self.agents[agent_name] = agent_w\n","            for k, v in agent.get_famparams().items():\n","                self.history.record_metric(k, v)\n","\n","\n","    def period(self):\n","        self.history.start_local_time()\n","        self._period_count+=1\n","\n","        if self.period_duration_unity== \"seconde\":\n","            print(f\"\\n{self.name}, period {self._period_count}, each agent turns {self.periode_duration} secondes:\", end=\"\")\n","        else:\n","            print(f\"\\n{self.name}, period {self._period_count}, each agent turns {self.periode_duration} steps:\", end=\"\")\n","\n","        for agent_w in self.agents.values():\n","            self._period_one_agent(agent_w)\n","\n","        if len(self.agents)>1:\n","            self.mutation()\n","        self.history.stop_local_time()\n","\n","\n","    def _period_one_agent(self, agent_w:Agent_wraper):\n","\n","        ti0=time.time()\n","        optimization_step=0\n","        ok=True\n","        while ok:\n","            optimization_step+=1\n","            if self.period_duration_unity== \"seconde\":\n","                ok= time.time() - ti0 < self.periode_duration\n","            else:\n","                ok=optimization_step<self.periode_duration\n","\n","            score=agent_w.agent.optimize_return_score()\n","            if np.isnan(score): score = -float(\"inf\")\n","\n","            self.history.record_metric(\"score\", score)\n","\n","            if  agent_w.best_score is None or score>agent_w.best_score:\n","                agent_w.save_at_best(score)\n","                print(\" \"+agent_w.name+\"↗\"+str(np.round(score,4)),end=\"\")\n","            else:\n","                print(\".\",end=\"\")\n","\n","        for k, v in agent_w.agent.to_register_at_period_end().items():\n","            self.history.record_metric(k, v)\n","\n","\n","    def mutation(self):\n","\n","        sorted_agents = sorted(self.agents.values(), key=lambda a_w: a_w.best_score)\n","        nb_weak=int( len(self.agents) * self.ratio_weak)\n","        nb_strong=int(len(self.agents) * self.ratio_strong)\n","        if nb_weak==0: nb_weak=1\n","        if nb_strong==0: nb_strong=1\n","\n","        nb_interessant_strong=[]\n","        for i in range(nb_strong):\n","            if sorted_agents[-i+1].best_score>self.min_interessant_score:\n","                nb_interessant_strong.append(sorted_agents[-i+1])\n","\n","        if len(nb_interessant_strong)==0:\n","            print(\"\\nATTENTION: aucun des agents n'a un score interessant: on perturbe la moitité des agents\" )\n","            for agent_w in self.agents.values():\n","                if np.random.random()>0.5:\n","                    agent_w.agent.perturb_famparams()\n","            return\n","\n","        weaks = sorted_agents[:nb_weak]\n","\n","        print(\", mutations:\", end=\"\")\n","        for i in range(nb_weak):\n","            weak = weaks[i]\n","            strong_index=np.random.randint(len(nb_interessant_strong))\n","            strong  = nb_interessant_strong[strong_index]\n","            \"\"\"ici on fait de l'early stopping en récupérant les meilleurs poids et meilleurs famparam du meilleurs agent.\n","               cela évite aussi d'avoir exactement deux fois le même agent.\n","            \"\"\"\n","            weak.load_from_another(strong)\n","            weak.agent.perturb_famparams()\n","\n","            for k, v in weak.agent.get_famparams().items():\n","                \"\"\"on ajoute un tag pour repérer ces nouveau poids \"\"\"\n","                self.history.record_metric(k, v)\n","            for k, v in weak.agent.weights_info_to_register().items():\n","                self.history.record_metric(k, v)\n","\n","            weak.mutation_names.append(strong.name)\n","            print(f\"{weak.get_name_suffixed()}|\", end=\"\")\n","\n","\n","    def plot_metric(self, metric: str, ax, transformation=None):\n","\n","        ax.set_xlabel(\"local time\")\n","        ax.set_ylabel(metric)\n","\n","        x = self.history.metrics_times[metric]\n","        y = self.history.metrics_values[metric]\n","        if transformation is not None:\n","            y=transformation(y)\n","\n","        flags=self.history.metrics_flags.get(metric,set())\n","        for i in range(len(x)):\n","            if i in flags:\n","                ax.plot(x[i],y[i],self.color+\"+\")\n","            else:\n","                ax.plot(x[i], y[i], self.color + \".\")\n","\n","    def plot_two_metrics(self,metric0:str,metric1:str,ax):\n","\n","        ax.set_xlabel(metric0)\n","        ax.set_ylabel(metric1)\n","\n","        x = self.history.metrics_values[metric0]\n","        y = self.history.metrics_values[metric1]\n","        assert len(x)==len(y),f\"les métricques {metric0} et {metric1} n'ont pas été enregistrée le même nombre de fois\"\n","\n","        flags0 = self.history.metrics_flags.get(metric0,set())\n","        flags1 = self.history.metrics_flags.get(metric0,set())\n","        flags=flags0.union(flags1)\n","        for i in range(len(x)):\n","            if i in flags:\n","                ax.plot(x[i], y[i], self.color + \"o\", alpha=i / len(x))\n","            else:\n","                ax.plot(x[i], y[i], self.color + \".\", alpha=i / len(x))\n","\n","\n","    def stats_of_best(self,nb_best=None):\n","        if nb_best is None:\n","            nb_best = int(len(self.agents) * self.ratio_strong)\n","        assert nb_best<=len(self.agents)\n","\n","        agent_w_sorted=sorted(self.agents.values(),key=lambda ag:ag.best_score)\n","        best:List[Agent_wraper]=agent_w_sorted[-nb_best-1:]\n","\n","        res=dict()\n","        sum_score=0\n","        for agent in best:\n","            sum_score+=agent.best_score\n","            for k,v in agent.best_famparams.items():\n","                res[k]=res.get(k,0)+v*agent.best_score\n","\n","        for k,v in res.items():\n","            res[k]/=sum_score\n","\n","        return res\n","\n","    def get_best_agent(self,mean_its_weights=True)->Abstract_Agent:\n","        agent_w_sorted = sorted(self.agents.values(), key=lambda ag: ag.best_score)\n","        best=agent_w_sorted[-1]\n","\n","        if mean_its_weights:\n","            weights=best.mean_of_best_weights()\n","        else:\n","            weights=best.best_weights[-1]\n","        best.agent.set_weights(weights)\n","\n","        return best.agent\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IQMUsH8pPAa1"},"source":["## DDQN\n","\n"]},{"cell_type":"code","metadata":{"id":"BZSacMOjPFtw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDRzCgrVPGM3"},"source":["## Entrainement"]},{"cell_type":"code","metadata":{"id":"dRiXzXn6PIor"},"source":[""],"execution_count":null,"outputs":[]}]}